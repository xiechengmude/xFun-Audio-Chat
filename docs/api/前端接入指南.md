# Fun-Audio-Chat 前端接入指南

> 版本: v1.0 | 更新时间: 2026-01-08

本文档提供三个核心接口的前端接入方式，包括 ASR（语音识别）、S2S（语音对话）、TTS（语音合成）。

---

## 目录

1. [服务端点概览](#1-服务端点概览)
2. [ASR 语音识别接口](#2-asr-语音识别接口)
3. [S2S 语音对话接口](#3-s2s-语音对话接口)
4. [TTS 语音合成接口](#4-tts-语音合成接口)
5. [通用工具函数](#5-通用工具函数)
6. [错误处理](#6-错误处理)
7. [完整示例项目](#7-完整示例项目)

---

## 1. 服务端点概览

### 1.1 端口映射

> **注意**: RunPod 部署时，内部端口会映射到外部端口。请根据实际部署环境配置。

| 服务 | 协议 | 内部端口 | 外部端口 (RunPod) | 端点 | 用途 |
|------|------|---------|------------------|------|------|
| S2S | WebSocket | 8002 | 22125 | `/api/chat` | 实时语音对话 |
| ASR | HTTP | 8003 | 22126 | `/api/transcribe` | 语音转文字 |
| TTS | HTTP | 8004 | 22127 | `/api/synthesize` | 文字转语音 |

### 1.2 当前部署实例 (RunPod)

```bash
Host: 69.30.85.123

# S2S 语音对话
ws://69.30.85.123:22125/api/chat

# ASR 语音识别
http://69.30.85.123:22126/api/transcribe
http://69.30.85.123:22126/health
http://69.30.85.123:22126/api/info

# TTS 语音合成 (需启用独立TTS服务)
http://69.30.85.123:22127/api/synthesize
http://69.30.85.123:22127/health
```

### 1.3 本地开发环境

```bash
# S2S 语音对话
ws://localhost:8002/api/chat

# ASR 语音识别
http://localhost:8003/api/transcribe

# TTS 语音合成
http://localhost:8004/api/synthesize
```

---

## 2. ASR 语音识别接口

### 2.1 接口说明

| 项目 | 说明 |
|------|------|
| 端点 | `POST /api/transcribe` |
| 内部端口 | 8003 |
| 外部端口 (RunPod) | 22126 |
| Content-Type | `multipart/form-data` |
| 支持格式 | WAV, MP3, FLAC, OGG 等 |
| 支持语言 | 中文、英文、日文 |
| 并发支持 | 10 workers |

### 2.2 请求参数

| 参数 | 类型 | 必填 | 说明 |
|------|------|------|------|
| audio | File | 是 | 音频文件 |
| language | string | 否 | 识别语言，默认 "中文" |
| hotwords | string | 否 | 热词列表，逗号分隔 |

### 2.3 响应格式

```json
{
  "text": "识别出的文本内容",
  "language": "中文",
  "success": true
}
```

### 2.4 JavaScript/TypeScript 实现

```typescript
// types.ts
interface ASRResponse {
  text: string;
  language: string;
  success: boolean;
  error?: string;
}

interface ASROptions {
  language?: '中文' | '英文' | '日文';
  hotwords?: string[];
}

// asr-client.ts
class ASRClient {
  private baseUrl: string;

  constructor(baseUrl: string = 'http://localhost:8003') {
    this.baseUrl = baseUrl;
  }

  /**
   * 转写音频文件
   */
  async transcribe(
    audioFile: File | Blob,
    options: ASROptions = {}
  ): Promise<ASRResponse> {
    const formData = new FormData();
    formData.append('audio', audioFile);

    if (options.language) {
      formData.append('language', options.language);
    }

    if (options.hotwords && options.hotwords.length > 0) {
      formData.append('hotwords', options.hotwords.join(','));
    }

    const response = await fetch(`${this.baseUrl}/api/transcribe`, {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      throw new Error(`ASR request failed: ${response.statusText}`);
    }

    return response.json();
  }

  /**
   * 从 MediaRecorder 录制的 Blob 转写
   */
  async transcribeBlob(
    blob: Blob,
    options: ASROptions = {}
  ): Promise<ASRResponse> {
    const file = new File([blob], 'recording.wav', { type: blob.type });
    return this.transcribe(file, options);
  }

  /**
   * 健康检查
   */
  async healthCheck(): Promise<boolean> {
    try {
      const response = await fetch(`${this.baseUrl}/health`);
      const data = await response.json();
      return data.status === 'healthy';
    } catch {
      return false;
    }
  }
}

export { ASRClient, ASRResponse, ASROptions };
```

### 2.5 React Hook 示例

```tsx
// useASR.ts
import { useState, useCallback, useRef } from 'react';
import { ASRClient, ASRResponse, ASROptions } from './asr-client';

// RunPod 部署地址
const asrClient = new ASRClient('http://69.30.85.123:22126');

export function useASR() {
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [result, setResult] = useState<ASRResponse | null>(null);

  const transcribe = useCallback(async (
    audio: File | Blob,
    options?: ASROptions
  ) => {
    setIsLoading(true);
    setError(null);

    try {
      const response = await asrClient.transcribe(
        audio instanceof Blob ? new File([audio], 'audio.wav') : audio,
        options
      );
      setResult(response);
      return response;
    } catch (err) {
      const message = err instanceof Error ? err.message : '识别失败';
      setError(message);
      throw err;
    } finally {
      setIsLoading(false);
    }
  }, []);

  return { transcribe, isLoading, error, result };
}

// 使用示例
function VoiceInput() {
  const { transcribe, isLoading, result } = useASR();
  const [recording, setRecording] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);

  const startRecording = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const mediaRecorder = new MediaRecorder(stream);
    mediaRecorderRef.current = mediaRecorder;
    chunksRef.current = [];

    mediaRecorder.ondataavailable = (e) => {
      if (e.data.size > 0) {
        chunksRef.current.push(e.data);
      }
    };

    mediaRecorder.onstop = async () => {
      const blob = new Blob(chunksRef.current, { type: 'audio/wav' });
      await transcribe(blob, { language: '中文' });
      stream.getTracks().forEach(track => track.stop());
    };

    mediaRecorder.start();
    setRecording(true);
  };

  const stopRecording = () => {
    mediaRecorderRef.current?.stop();
    setRecording(false);
  };

  return (
    <div>
      <button
        onClick={recording ? stopRecording : startRecording}
        disabled={isLoading}
      >
        {recording ? '停止录音' : '开始录音'}
      </button>
      {isLoading && <p>识别中...</p>}
      {result && <p>识别结果: {result.text}</p>}
    </div>
  );
}
```

### 2.6 cURL 测试命令

```bash
# RunPod 部署 - 基础调用
curl -X POST http://69.30.85.123:22126/api/transcribe \
  -F 'audio=@audio.wav' \
  -F 'language=中文'

# RunPod 部署 - 带热词
curl -X POST http://69.30.85.123:22126/api/transcribe \
  -F 'audio=@audio.wav' \
  -F 'language=中文' \
  -F 'hotwords=人工智能,机器学习,深度学习'

# 健康检查
curl http://69.30.85.123:22126/health

# 本地开发
curl -X POST http://localhost:8003/api/transcribe \
  -F 'audio=@audio.wav'
```

---

## 3. S2S 语音对话接口

### 3.1 接口说明

| 项目 | 说明 |
|------|------|
| 端点 | `ws://host:port/api/chat` |
| 内部端口 | 8002 |
| 外部端口 (RunPod) | 22125 |
| 协议 | WebSocket (Binary) |
| 音频格式 | Opus 编码, 24kHz |
| 特性 | 双向实时通信、流式响应 |

### 3.2 消息协议

#### 消息类型 (第一个字节)

| 类型码 | 名称 | 方向 | 说明 |
|--------|------|------|------|
| 0x00 | HANDSHAKE | S→C | 握手响应 |
| 0x01 | AUDIO | 双向 | 音频数据 (Opus) |
| 0x02 | TEXT | S→C | 文本内容 |
| 0x03 | CONTROL | C→S | 控制信号 |
| 0x04 | METADATA | C→S | 元数据 (如自定义 system prompt) |
| 0x05 | ERROR | S→C | 错误信息 |
| 0x06 | PING | C→S | 心跳 |

#### 控制信号 (0x03 后的第二个字节)

| 控制码 | 名称 | 说明 |
|--------|------|------|
| 0x00 | START | 开始新一轮对话 |
| 0x01 | END_TURN | 结束当前轮次 |
| 0x02 | PAUSE | 暂停录音 |
| 0x03 | RESTART | 重新开始 |

### 3.3 通信流程

```
客户端                                     服务端
   |                                          |
   |  --------- WebSocket 连接 ---------->    |
   |  <------- 0x00 握手响应 -------------    |
   |                                          |
   |  --------- 0x03 0x00 (START) -------->   |
   |  --------- 0x01 [Opus音频] ---------->   |
   |  --------- 0x01 [Opus音频] ---------->   |
   |  ...                                     |
   |  --------- 0x03 0x02 (PAUSE) -------->   |
   |                                          |
   |  <------- 0x02 [处理中...] -----------   |
   |  <------- 0x01 [Opus音频] ------------   |
   |  <------- 0x02 [回复文本] ------------   |
   |  <------- 0x01 [Opus音频] ------------   |
   |  ...                                     |
```

### 3.4 TypeScript 实现

```typescript
// types.ts
interface S2SOptions {
  onAudio?: (audioData: ArrayBuffer) => void;
  onText?: (text: string) => void;
  onError?: (error: string) => void;
  onStateChange?: (state: ConnectionState) => void;
  systemPrompt?: string;
}

type ConnectionState = 'connecting' | 'connected' | 'recording' | 'processing' | 'responding' | 'disconnected';

// s2s-client.ts
class S2SClient {
  private ws: WebSocket | null = null;
  private audioContext: AudioContext | null = null;
  private state: ConnectionState = 'disconnected';
  private options: S2SOptions;
  private wsUrl: string;

  constructor(wsUrl: string, options: S2SOptions = {}) {
    this.wsUrl = wsUrl;
    this.options = options;
  }

  /**
   * 连接到服务器
   */
  async connect(): Promise<void> {
    return new Promise((resolve, reject) => {
      this.setState('connecting');

      this.ws = new WebSocket(this.wsUrl);
      this.ws.binaryType = 'arraybuffer';

      this.ws.onopen = () => {
        console.log('WebSocket connected');
      };

      this.ws.onmessage = (event) => {
        this.handleMessage(event.data);
        if (this.state === 'connecting') {
          this.setState('connected');
          resolve();
        }
      };

      this.ws.onerror = (error) => {
        console.error('WebSocket error:', error);
        reject(error);
      };

      this.ws.onclose = () => {
        this.setState('disconnected');
      };

      setTimeout(() => {
        if (this.state === 'connecting') {
          reject(new Error('Connection timeout'));
        }
      }, 10000);
    });
  }

  /**
   * 处理收到的消息
   */
  private handleMessage(data: ArrayBuffer): void {
    const view = new Uint8Array(data);
    const msgType = view[0];
    const payload = view.slice(1);

    switch (msgType) {
      case 0x00: // HANDSHAKE
        console.log('Received handshake');
        break;

      case 0x01: // AUDIO
        this.setState('responding');
        this.options.onAudio?.(payload.buffer);
        break;

      case 0x02: // TEXT
        const text = new TextDecoder().decode(payload);
        this.options.onText?.(text);
        break;

      case 0x05: // ERROR
        const errorMsg = new TextDecoder().decode(payload);
        this.options.onError?.(errorMsg);
        break;
    }
  }

  /**
   * 发送控制信号
   */
  private sendControl(action: number): void {
    if (this.ws?.readyState === WebSocket.OPEN) {
      const msg = new Uint8Array([0x03, action]);
      this.ws.send(msg);
    }
  }

  /**
   * 发送元数据
   */
  sendMetadata(metadata: object): void {
    if (this.ws?.readyState === WebSocket.OPEN) {
      const jsonStr = JSON.stringify(metadata);
      const encoder = new TextEncoder();
      const payload = encoder.encode(jsonStr);
      const msg = new Uint8Array(1 + payload.length);
      msg[0] = 0x04;
      msg.set(payload, 1);
      this.ws.send(msg);
    }
  }

  /**
   * 设置自定义 system prompt
   */
  setSystemPrompt(prompt: string): void {
    this.sendMetadata({ system_prompt: prompt });
  }

  /**
   * 开始新一轮对话
   */
  startTurn(): void {
    this.sendControl(0x00); // START
    this.setState('recording');
  }

  /**
   * 结束当前轮次
   */
  endTurn(): void {
    this.sendControl(0x02); // PAUSE
    this.setState('processing');
  }

  /**
   * 发送音频数据 (Opus 编码)
   */
  sendAudio(opusData: Uint8Array): void {
    if (this.ws?.readyState === WebSocket.OPEN && this.state === 'recording') {
      const msg = new Uint8Array(1 + opusData.length);
      msg[0] = 0x01;
      msg.set(opusData, 1);
      this.ws.send(msg);
    }
  }

  /**
   * 设置状态
   */
  private setState(state: ConnectionState): void {
    this.state = state;
    this.options.onStateChange?.(state);
  }

  /**
   * 断开连接
   */
  disconnect(): void {
    this.ws?.close();
    this.setState('disconnected');
  }

  getState(): ConnectionState {
    return this.state;
  }
}

export { S2SClient, S2SOptions, ConnectionState };
```

### 3.5 React 组件示例

```tsx
// VoiceChat.tsx
import { useState, useRef, useEffect } from 'react';
import { S2SClient, ConnectionState } from './s2s-client';

interface Props {
  // RunPod 部署地址
  serverUrl?: string;
  systemPrompt?: string;
}

export function VoiceChat({
  serverUrl = 'ws://69.30.85.123:22125/api/chat',
  systemPrompt
}: Props) {
  const [state, setState] = useState<ConnectionState>('disconnected');
  const [messages, setMessages] = useState<Array<{role: string, content: string}>>([]);
  const [isRecording, setIsRecording] = useState(false);
  const clientRef = useRef<S2SClient | null>(null);

  useEffect(() => {
    clientRef.current = new S2SClient(serverUrl, {
      onAudio: (data) => {
        // 播放音频
        playOpusAudio(data);
      },
      onText: (text) => {
        if (!text.startsWith('[')) {
          setMessages(prev => {
            const last = prev[prev.length - 1];
            if (last?.role === 'assistant') {
              return [...prev.slice(0, -1), { role: 'assistant', content: last.content + text }];
            }
            return [...prev, { role: 'assistant', content: text }];
          });
        }
      },
      onStateChange: setState,
    });

    return () => clientRef.current?.disconnect();
  }, [serverUrl]);

  const connect = async () => {
    await clientRef.current?.connect();
    if (systemPrompt) {
      clientRef.current?.setSystemPrompt(systemPrompt);
    }
  };

  const startRecording = () => {
    setIsRecording(true);
    clientRef.current?.startTurn();
    // 开始录音并发送...
  };

  const stopRecording = () => {
    setIsRecording(false);
    clientRef.current?.endTurn();
  };

  return (
    <div className="voice-chat">
      <div>状态: {state}</div>
      <div className="messages">
        {messages.map((msg, i) => (
          <div key={i} className={msg.role}>
            <strong>{msg.role === 'user' ? '你' : 'AI'}:</strong> {msg.content}
          </div>
        ))}
      </div>
      <div className="controls">
        {state === 'disconnected' ? (
          <button onClick={connect}>连接</button>
        ) : (
          <button
            onMouseDown={startRecording}
            onMouseUp={stopRecording}
            disabled={state === 'processing'}
          >
            {isRecording ? '松开发送' : '按住说话'}
          </button>
        )}
      </div>
    </div>
  );
}

async function playOpusAudio(data: ArrayBuffer) {
  // 使用 opus-decoder 解码并播放
  // ...
}
```

### 3.6 测试连接

```javascript
// 浏览器控制台测试
const ws = new WebSocket('ws://69.30.85.123:22125/api/chat');
ws.binaryType = 'arraybuffer';
ws.onopen = () => console.log('Connected');
ws.onmessage = (e) => {
  const view = new Uint8Array(e.data);
  console.log('Message type:', view[0], 'Length:', view.length);
};
```

---

## 4. TTS 语音合成接口

### 4.1 接口说明

| 端点 | 方法 | 说明 |
|------|------|------|
| `/api/synthesize` | POST | 返回 Base64 音频 |
| `/api/synthesize/file` | POST | 返回 WAV 文件 |
| `/health` | GET | 健康检查 |
| `/api/info` | GET | 模型信息 |

| 项目 | 说明 |
|------|------|
| 内部端口 | 8004 |
| 外部端口 (RunPod) | 22127 |
| 采样率 | 24000 Hz |
| 注意 | 需要启用独立 TTS 服务 |

> **注意**: 当前部署中，TTS 已内置在 S2S 服务中。独立 TTS 服务需要使用 `--with-tts` 参数启动。

### 4.2 请求参数

```json
{
  "text": "要合成的文本",
  "speaker_id": "中文女",
  "prompt_wav": "/path/to/ref.wav",
  "prompt_text": "参考音频对应的文本",
  "stream": false
}
```

### 4.3 响应格式

```json
{
  "audio": "<base64_wav_data>",
  "sample_rate": 24000,
  "duration": 3.5,
  "success": true
}
```

### 4.4 TypeScript 实现

```typescript
// tts-client.ts
interface TTSResponse {
  audio: string;
  sample_rate: number;
  duration: number;
  success: boolean;
  error?: string;
}

interface TTSOptions {
  speakerId?: string;
  promptWav?: string;
  promptText?: string;
  stream?: boolean;
}

class TTSClient {
  private baseUrl: string;
  private audioContext: AudioContext | null = null;

  // RunPod 部署地址
  constructor(baseUrl: string = 'http://69.30.85.123:22127') {
    this.baseUrl = baseUrl;
  }

  /**
   * 合成语音 (返回 Base64)
   */
  async synthesize(text: string, options: TTSOptions = {}): Promise<TTSResponse> {
    const response = await fetch(`${this.baseUrl}/api/synthesize`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        text,
        speaker_id: options.speakerId || '中文女',
        prompt_wav: options.promptWav,
        prompt_text: options.promptText,
        stream: false,
      }),
    });

    if (!response.ok) {
      throw new Error(`TTS request failed: ${response.statusText}`);
    }

    return response.json();
  }

  /**
   * 合成语音并直接获取 Blob
   */
  async synthesizeToBlob(text: string, options: TTSOptions = {}): Promise<Blob> {
    const response = await fetch(`${this.baseUrl}/api/synthesize/file`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        text,
        speaker_id: options.speakerId || '中文女',
      }),
    });

    if (!response.ok) {
      throw new Error(`TTS request failed: ${response.statusText}`);
    }

    return response.blob();
  }

  /**
   * 合成并播放
   */
  async synthesizeAndPlay(text: string, options: TTSOptions = {}): Promise<void> {
    const result = await this.synthesize(text, options);
    await this.playBase64Audio(result.audio);
  }

  /**
   * 播放 Base64 音频
   */
  async playBase64Audio(base64Audio: string): Promise<void> {
    const binaryString = atob(base64Audio);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }

    if (!this.audioContext) {
      this.audioContext = new AudioContext();
    }

    const audioBuffer = await this.audioContext.decodeAudioData(bytes.buffer);
    const source = this.audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(this.audioContext.destination);
    source.start();

    return new Promise(resolve => {
      source.onended = () => resolve();
    });
  }

  /**
   * 下载音频文件
   */
  async downloadAudio(text: string, filename: string = 'speech.wav'): Promise<void> {
    const blob = await this.synthesizeToBlob(text);
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = filename;
    a.click();
    URL.revokeObjectURL(url);
  }

  /**
   * 健康检查
   */
  async healthCheck(): Promise<boolean> {
    try {
      const response = await fetch(`${this.baseUrl}/health`);
      const data = await response.json();
      return data.status === 'healthy';
    } catch {
      return false;
    }
  }
}

export { TTSClient, TTSResponse, TTSOptions };
```

### 4.5 React Hook 示例

```tsx
// useTTS.ts
import { useState, useCallback } from 'react';
import { TTSClient, TTSOptions } from './tts-client';

const ttsClient = new TTSClient('http://69.30.85.123:22127');

export function useTTS() {
  const [isLoading, setIsLoading] = useState(false);
  const [isPlaying, setIsPlaying] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const speak = useCallback(async (text: string, options?: TTSOptions) => {
    setIsLoading(true);
    setError(null);

    try {
      setIsPlaying(true);
      await ttsClient.synthesizeAndPlay(text, options);
    } catch (err) {
      setError(err instanceof Error ? err.message : '合成失败');
      throw err;
    } finally {
      setIsLoading(false);
      setIsPlaying(false);
    }
  }, []);

  const download = useCallback(async (text: string, filename?: string) => {
    setIsLoading(true);
    try {
      await ttsClient.downloadAudio(text, filename);
    } catch (err) {
      setError(err instanceof Error ? err.message : '下载失败');
    } finally {
      setIsLoading(false);
    }
  }, []);

  return { speak, download, isLoading, isPlaying, error };
}
```

### 4.6 cURL 测试命令

```bash
# RunPod 部署 - 合成并返回 Base64
curl -X POST http://69.30.85.123:22127/api/synthesize \
  -H "Content-Type: application/json" \
  -d '{"text": "你好，欢迎使用语音合成服务", "speaker_id": "中文女"}'

# RunPod 部署 - 合成并下载 WAV 文件
curl -X POST http://69.30.85.123:22127/api/synthesize/file \
  -H "Content-Type: application/json" \
  -d '{"text": "你好世界"}' \
  -o output.wav

# 健康检查
curl http://69.30.85.123:22127/health

# 本地开发
curl -X POST http://localhost:8004/api/synthesize \
  -H "Content-Type: application/json" \
  -d '{"text": "测试语音合成"}'
```

---

## 5. 通用工具函数

### 5.1 音频录制工具

```typescript
// audio-recorder.ts
export class AudioRecorder {
  private mediaRecorder: MediaRecorder | null = null;
  private chunks: Blob[] = [];
  private stream: MediaStream | null = null;

  async start(): Promise<void> {
    this.stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        sampleRate: 16000,
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
      }
    });

    this.chunks = [];
    this.mediaRecorder = new MediaRecorder(this.stream, {
      mimeType: 'audio/webm;codecs=opus'
    });

    this.mediaRecorder.ondataavailable = (e) => {
      if (e.data.size > 0) this.chunks.push(e.data);
    };

    this.mediaRecorder.start(100);
  }

  async stop(): Promise<Blob> {
    return new Promise((resolve) => {
      if (!this.mediaRecorder) throw new Error('Not recording');

      this.mediaRecorder.onstop = () => {
        const blob = new Blob(this.chunks, { type: 'audio/webm' });
        this.cleanup();
        resolve(blob);
      };

      this.mediaRecorder.stop();
    });
  }

  private cleanup(): void {
    this.stream?.getTracks().forEach(track => track.stop());
    this.stream = null;
    this.mediaRecorder = null;
    this.chunks = [];
  }

  isRecording(): boolean {
    return this.mediaRecorder?.state === 'recording';
  }
}
```

### 5.2 音频格式转换

```typescript
// audio-utils.ts
export async function webmToWav(webmBlob: Blob): Promise<Blob> {
  const audioContext = new AudioContext();
  const arrayBuffer = await webmBlob.arrayBuffer();
  const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
  const wavBuffer = audioBufferToWav(audioBuffer);
  return new Blob([wavBuffer], { type: 'audio/wav' });
}

function audioBufferToWav(buffer: AudioBuffer): ArrayBuffer {
  const numChannels = 1;
  const sampleRate = buffer.sampleRate;
  const bytesPerSample = 2;
  const dataLength = buffer.length * bytesPerSample;
  const headerLength = 44;

  const arrayBuffer = new ArrayBuffer(headerLength + dataLength);
  const view = new DataView(arrayBuffer);

  // WAV header
  writeString(view, 0, 'RIFF');
  view.setUint32(4, 36 + dataLength, true);
  writeString(view, 8, 'WAVE');
  writeString(view, 12, 'fmt ');
  view.setUint32(16, 16, true);
  view.setUint16(20, 1, true);
  view.setUint16(22, numChannels, true);
  view.setUint32(24, sampleRate, true);
  view.setUint32(28, sampleRate * bytesPerSample, true);
  view.setUint16(32, bytesPerSample, true);
  view.setUint16(34, 16, true);
  writeString(view, 36, 'data');
  view.setUint32(40, dataLength, true);

  const channelData = buffer.getChannelData(0);
  let offset = 44;
  for (let i = 0; i < buffer.length; i++) {
    view.setInt16(offset, channelData[i] * 0x7FFF, true);
    offset += 2;
  }

  return arrayBuffer;
}

function writeString(view: DataView, offset: number, str: string): void {
  for (let i = 0; i < str.length; i++) {
    view.setUint8(offset + i, str.charCodeAt(i));
  }
}
```

---

## 6. 错误处理

```typescript
// errors.ts
export class AudioServiceError extends Error {
  constructor(
    message: string,
    public code: string,
    public service: 'ASR' | 'S2S' | 'TTS'
  ) {
    super(message);
    this.name = 'AudioServiceError';
  }
}

export const ErrorCodes = {
  NETWORK_ERROR: 'NETWORK_ERROR',
  TIMEOUT: 'TIMEOUT',
  INVALID_AUDIO: 'INVALID_AUDIO',
  PERMISSION_DENIED: 'PERMISSION_DENIED',
} as const;

// 使用示例
try {
  await asrClient.transcribe(audioBlob);
} catch (error) {
  if (error instanceof AudioServiceError) {
    console.error(`[${error.service}] ${error.code}: ${error.message}`);
  }
}
```

---

## 7. 完整示例项目

### 7.1 环境配置

```typescript
// config.ts
export const config = {
  // RunPod 部署
  ASR_URL: 'http://69.30.85.123:22126',
  S2S_URL: 'ws://69.30.85.123:22125',
  TTS_URL: 'http://69.30.85.123:22127',

  // 本地开发
  // ASR_URL: 'http://localhost:8003',
  // S2S_URL: 'ws://localhost:8002',
  // TTS_URL: 'http://localhost:8004',
};
```

### 7.2 依赖安装

```bash
npm install opus-recorder @piotr-niciak/opus-decoder
```

---

## 附录: 服务端点快速参考

### RunPod 部署 (当前实例)

```
Host: 69.30.85.123

# ASR 语音识别
POST http://69.30.85.123:22126/api/transcribe
GET  http://69.30.85.123:22126/health
GET  http://69.30.85.123:22126/api/info

# S2S 语音对话
WS   ws://69.30.85.123:22125/api/chat

# TTS 语音合成 (需启用独立服务)
POST http://69.30.85.123:22127/api/synthesize
POST http://69.30.85.123:22127/api/synthesize/file
GET  http://69.30.85.123:22127/health
```

### 本地开发

```
# ASR 语音识别
POST http://localhost:8003/api/transcribe

# S2S 语音对话
WS   ws://localhost:8002/api/chat

# TTS 语音合成
POST http://localhost:8004/api/synthesize
```

---

*文档版本: v1.0 | 最后更新: 2026-01-08*
