{
    "pages": [
        {
            "page_number": 1,
            "text": "A partial orthogonalization method for simulating covariance and concentration graph matrices\n\nIrene C\u00f3rdoba1\nGherardo Varando1,2\nConcha Bielza1\nPedro Larra\u00f1aga1\n\n1Department of Artificial Intelligence, Universidad Polit\u00e9cnica de Madrid (Spain)\n2Department of Mathematical Sciences, University of Copenhagen (Denmark)*\n\nAbstract\n\nStructure learning methods for covariance and concentration graphs are often validated on synthetic models, usually obtained by randomly generating: (i) an undirected graph, and (ii) a compatible symmetric positive definite (SPD) matrix. In order to ensure positive definiteness in (ii), a dominant diagonal is usually imposed. However, the link strengths in the resulting graphical model, determined by off-diagonal entries in the SPD matrix, are in many scenarios extremely weak. Recovering the structure of the undirected graph thus becomes a challenge, and algorithm validation is notably affected. In this paper, we propose an alternative method which overcomes such problem yet yielding a compatible SPD matrix. We generate a partially row-wise-orthogonal matrix factor, where pairwise orthogonal rows correspond to missing edges in the undirected graph. In numerical experiments ranging from moderately dense to sparse scenarios, we obtain that, as the dimension increases, the link strength we simulate is stable with respect to the structure sparsity. Importantly, we show in a real validation setting how structure recovery is greatly improved for all learning algorithms when using our proposed method, thereby producing a more realistic comparison framework.\n\nKeywords: Concentration graph, covariance graph, positive definite matrix simulation, undirected graphical model, algorithm validation.\n\n1. Introduction\n\nStructure learning algorithms in graphical models are validated using either benchmark or randomly generated synthetic models from which data is sampled. This allows to evaluate their performance by comparing the recovered graph, obtained by running the algorithm over the generated data, with the known true structure. The synthetic graphical models are typically constructed in a two-step manner: a graph structure is selected at random or chosen so that it is representative of the problem at hand; and, similarly, its parameters are fixed or randomly sampled.\n\nCovariance (Cox and Wermuth, 1993; Kauermann, 1996) and concentration graphs (Dempster, 1972; Lauritzen, 1996) are graphical models where the variables are assumed to follow a multivariate Gaussian distribution, and the structure is directly read off in the covariance or concentration matrix, respectively. Looking at the literature on these models, one finds that typical benchmark structures are Toeplitz, banded, diagonally spiked and block diagonal covariance or concentration matrices (Yuan and Lin, 2007; Xue and Zou, 2012; Ledoit and Wolf, 2012), with parameters fixed to ensure positive definiteness.\n\n*. Starting August 2018",
            "regions": [],
            "region_count": 0,
            "processing_time": 4.416,
            "width": 1582,
            "height": 2048
        },
        {
            "page_number": 2,
            "text": "The issue of positive definiteness is specially relevant when the structure is randomly generated. One approach to overcome this could be to sample from a matrix distribution with support over the symmetric positive definite matrices compatible with the undirected graph structure. The hyper Wishart distributions (Dawid and Lauritzen, 1993; Letac and Massam, 2007) are the most well-developed in this sense, since they form a conjugate family for Bayesian analysis. However, while sampling algorithms are available for general concentration graphs (Carvalho et al., 2007; Lenkoski, 2013), in covariance graphs they have been developed only in the decomposable case (Khare and Rajaratnam, 2011).\n\nIn general, hyper Wishart distributions are rarely used in validation scenarios (Williams et al., 2018), and instead in the literature the most common approach to ensure positive definiteness is to enforce diagonal dominance in the covariance or concentration matrix (Lin et al., 2009; Arvaniti and Claassen, 2914; Stojkovic et al., 2017). However, when the undirected graph is moderately dense, the off-diagonal elements in the generated matrices, often interpreted as link strengths, are extremely small with respect to the diagonal entries and structure recovery becomes a challenge, thereby compromising the structure learning algorithm validation (Sch\u00e4fer and Strimmer, 2005a,b; Kr\u00e4mer et al., 2009; Cai et al., 2011). In this paper, we propose an alternative method to overcome this problem based on partial orthogonalizations. In particular, we build a matrix factor where pairwise orthogonal rows correspond to missing edges in the undirected graph. Our method does not suffer from the problem of weak link strengths, as we numerically check in a wide range of sparsity scenarios. We also use our simulation method in a real validation setting and show how the performance is greatly improved for every learning algorithm, thereby potentially changing the conclusions drawn if only using diagonally dominant matrices for comparison.\n\nThe rest of the paper is organized as follows. Preliminaries are introduced in Section 2, where we briefly overview concentration and covariance graphs, and the main characteristics of diagonally controlled matrices. Next, in Section 3, we present our partial orthogonalization method, analyzing its main properties and our particular implementation. Section 4 contains a description of the experiment set-up we have considered, and the interpretation of the results obtained. Finally, in Section 5 we conclude the paper and outline our plans for future research.\n\n2. Preliminaries\n\nIn the remainder of the paper, we will use the following notation. We let $X_1, \\ldots, X_p$ denote $p$ random variables and $X$ the random vector they form. For each subset $I \\subseteq \\{1, \\ldots, p\\}$, $X_I$ will be the subvector of $X$ indexed by $I$, that is, $(X_i)_{i \\in I}$. We follow Dawid (1980) and abbreviate conditional independence in the joint distribution of $X$ as $X_I \\perp X_J \\mid X_K$, meaning that $X_I$ is conditionally independent of $X_J$ given $X_K$, with $I, J, K$ pairwise disjoint subsets of indices. Entries in a matrix are denoted with the respective lower case letter, for example, $m_{ij}$ denotes the $(i,j)$ entry in matrix $M$.\n\n2.1 Gaussian graphical models\n\nCovariance and concentration graphs are graphical models where it is assumed that the statistical independences in the distribution of a multivariate Gaussian random vector $X = (X_1, \\ldots, X_p)$ can be represented by an undirected graph $G = (V, E)$. Typically, $X$ is assumed to have zero mean for lighter notation, and $V = \\{1, \\ldots, p\\}$ so that it indexes the random vector, that is, $X_V = X$. We will represent the edge set $E$ as a subset of $V \\times V$, therefore $(i,j) \\in E$ if and only if $(j,i) \\in E$.",
            "regions": [],
            "region_count": 0,
            "processing_time": 5.375,
            "width": 1582,
            "height": 2048
        }
    ],
    "total_pages": 12,
    "parsed_pages": 2,
    "total_time": 6.483,
    "throughput": "0.31 pgs/s"
}
